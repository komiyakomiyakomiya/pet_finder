{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/keras/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import feather\n",
    "import pandas as pd\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPooling1D, concatenate, BatchNormalization\n",
    "from keras.layers import Reshape, Flatten, Concatenate, SpatialDropout1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam, Optimizer\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from pymagnitude import *\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "def analyzer(text):\n",
    "    stop_words = ['i', 'a', 'an', 'the', 'to', 'and', 'or', 'if', 'is', 'are', 'am', 'it', 'this', 'that', 'of', 'from', 'in', 'on']\n",
    "    text = text.lower() # 小文字化\n",
    "    text = text.replace('\\n', '') # 改行削除\n",
    "    text = text.replace('\\t', '') # タブ削除\n",
    "    text = re.sub(re.compile(r'[!-\\/:-@[-`{-~]'), ' ', text) # 記号をスペースに置き換え\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "    text = [stemmer.stem(t) for t in text]\n",
    "    \n",
    "    words = []\n",
    "    for word in text:\n",
    "        if (re.compile(r'^.*[0-9]+.*$').fullmatch(word) is not None): # 数字が含まれるものは除外\n",
    "            continue\n",
    "        if word in stop_words: # ストップワードに含まれるものは除外\n",
    "            continue\n",
    "        if len(word) < 2: #  1文字、0文字（空文字）は除外\n",
    "            continue\n",
    "        words.append(word)\n",
    "        \n",
    "    return \" \".join(words)\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "    \n",
    "def to_bins(x, borders):\n",
    "    for i in range(len(borders)):\n",
    "        if x <= borders[i]:\n",
    "            return i\n",
    "    return len(borders)\n",
    "    \n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _loss(self, coef, X, y, idx):\n",
    "        X_p = np.array([to_bins(pred, coef) for pred in X])\n",
    "        ll = -get_score(y, X_p)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        coef = [1.5, 2.0, 2.5, 3.0]\n",
    "        golden1 = 0.618\n",
    "        golden2 = 1 - golden1\n",
    "        ab_start = [(1, 2), (1.5, 2.5), (2, 3), (2.5, 3.5)]\n",
    "        for it1 in range(10):\n",
    "            for idx in range(4):\n",
    "                # golden section search\n",
    "                a, b = ab_start[idx]\n",
    "                # calc losses\n",
    "                coef[idx] = a\n",
    "                la = self._loss(coef, X, y, idx)\n",
    "                coef[idx] = b\n",
    "                lb = self._loss(coef, X, y, idx)\n",
    "                for it in range(20):\n",
    "                    # choose value\n",
    "                    if la > lb:\n",
    "                        a = b - (b - a) * golden1\n",
    "                        coef[idx] = a\n",
    "                        la = self._loss(coef, X, y, idx)\n",
    "                    else:\n",
    "                        b = b - (b - a) * golden2\n",
    "                        coef[idx] = b\n",
    "                        lb = self._loss(coef, X, y, idx)\n",
    "        self.coef_ = {'x': coef}\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.array([to_bins(pred, coef) for pred in X])\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "    \n",
    "def get_keras_data(df, description_embeds):\n",
    "    X = {\n",
    "        \"numerical\": df[numerical].values,\n",
    "        \"description\": description_embeds,\n",
    "        \"img\": df[img_cols]\n",
    "    }\n",
    "    for c in categorical_features:\n",
    "        X[c] = df[c]\n",
    "    return X\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y-y_pred), axis=-1))\n",
    "\n",
    "def get_model(max_features, embedding_dim, emb_n=5, dout=.4):\n",
    "    inp_cats = []\n",
    "    embs = []\n",
    "    for c in categorical_features:\n",
    "        inp_cat = Input(shape=[1], name=c)\n",
    "        inp_cats.append(inp_cat)\n",
    "        embs.append((Embedding(X_train[c].max()+1, emb_n)(inp_cat)))\n",
    "    cats = Flatten()(concatenate(embs))\n",
    "    cats = Dense(8, activation=\"relu\")(cats)\n",
    "    cats = Dropout(dout)(cats)\n",
    "    cats = BatchNormalization()(cats)\n",
    "    \n",
    "    inp_numerical =  Input(shape=(len(numerical),), name=\"numerical\")\n",
    "    nums = Dense(128, activation=\"relu\")(inp_numerical)\n",
    "    nums = Dropout(dout)(nums)\n",
    "    nums = BatchNormalization()(nums)\n",
    "    \n",
    "    inp_img =  Input(shape=(len(img_cols),), name=\"img\")\n",
    "    x_img = BatchNormalization()(inp_img)\n",
    "    \n",
    "    inp_desc = Input(shape=(max_features, embedding_dim), name=\"description\")\n",
    "    emb_desc = SpatialDropout1D(0.3)(inp_desc)\n",
    "    x1 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(emb_desc)\n",
    "    x2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "    avg_pool2 = GlobalAveragePooling1D()(x2)\n",
    "    conc = Concatenate()([max_pool1, max_pool2, avg_pool1, avg_pool2])\n",
    "    conc = BatchNormalization()(conc)\n",
    "    \n",
    "    x = concatenate([conc, x_img, nums, cats])\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dout/2)(x)\n",
    "    \n",
    "    out = Dense(1, activation=\"linear\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp_cats+[inp_numerical, inp_img, inp_desc], outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=rmse)\n",
    "    return model\n",
    "\n",
    "\n",
    "def w2v_pymagnitude_tonn(train_text, path, max_features):\n",
    "    train_corpus = [text_to_word_sequence(text) for text in train_text]\n",
    "    model = Magnitude(path)\n",
    "    embedding_dim = model.dim\n",
    "    \n",
    "    result = []\n",
    "    for text in train_corpus:\n",
    "        vec = []\n",
    "        for word in text:\n",
    "            try:\n",
    "                vec_ = model.query(word)\n",
    "            except:\n",
    "                continue\n",
    "            vec.append(vec_)\n",
    "        if len(vec) == 0:\n",
    "            vec = np.zeros((max_features, embedding_dim))\n",
    "        else:\n",
    "            vec_ = [[0 for i in range(300)] for _ in range(max_features-len(vec))]\n",
    "            vec_.extend(vec)\n",
    "            vec = np.array(vec_)[:max_features]\n",
    "            \n",
    "        result.append(vec)\n",
    "    \n",
    "    return np.array(result), embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = ['ratio_median_Age_groupby_Type_Breed1_Breed2',\n",
    " 'ratio_median_Age_groupby_Type_Breed1',\n",
    " 'BreedName_main_breed',\n",
    " 'crop_y_max',\n",
    " 'diff_var_Sterilized_groupby_RescuerID_State',\n",
    " 'annots_score_sum_median',\n",
    " 'Age_mul_Quantity',\n",
    " 'img_9',\n",
    " 'img_163',\n",
    " 'img_224',\n",
    " 'diff_mean_Fee_groupby_Type_Breed1_Breed2',\n",
    " 'ratio_count_Quantity_groupby_RescuerID',\n",
    " 'img_148',\n",
    " 'median_Age_groupby_RescuerID_State',\n",
    " 'diff_max_Quantity_groupby_State',\n",
    " 'img_141',\n",
    " 'var_Quantity_groupby_Type_Breed1_Breed2',\n",
    " 'img_135',\n",
    " 'gnvec155',\n",
    " 'diff_var_Age_groupby_MaturitySize',\n",
    " 'img_49',\n",
    " 'ratio_sum_Sterilized_groupby_State',\n",
    " 'mean_Sterilized_groupby_RescuerID_State',\n",
    " 'img_132',\n",
    " 'diff_mean_Age_groupby_Type_Breed1',\n",
    " 'img_223',\n",
    " 'img_172',\n",
    " 'img_232',\n",
    " 'mean_Fee_groupby_Type_Breed1',\n",
    " 'diff_mean_Fee_groupby_Type_Breed1',\n",
    " 'diff_var_Sterilized_groupby_RescuerID_Type',\n",
    " 'img_150',\n",
    " 'BreedName_second_breed',\n",
    " 'crop_y_mean',\n",
    " 'mean_Quantity_groupby_RescuerID_Type',\n",
    " 'img_189',\n",
    " 'img_157',\n",
    " 'ratio_count_Age_groupby_RescuerID_Type',\n",
    " 'img_56',\n",
    " 'img_164',\n",
    " 'img_46',\n",
    " 'var_MaturitySize_groupby_RescuerID_State',\n",
    " 'dog_cat_scores_sum_max',\n",
    " 'num_images_per_pet',\n",
    " 'glove_mag187',\n",
    " 'gnvec258',\n",
    " 'annots_score_sum_max',\n",
    " 'mean_Quantity_groupby_Type_Breed1_Breed2',\n",
    " 'annots_score_amax_max',\n",
    " 'diff_var_Fee_groupby_Type_Breed1',\n",
    " 'ratio_count_Age_groupby_RescuerID_State',\n",
    " 'var_MaturitySize_groupby_RescuerID_Type',\n",
    " 'img_233',\n",
    " 'img_142',\n",
    " 'crop_y_min',\n",
    " 'img_249',\n",
    " 'mean_Fee_groupby_Type_Breed1_Breed2',\n",
    " 'ratio_sum_Age_groupby_RescuerID',\n",
    " 'glove_mag177',\n",
    " 'diff_mean_Fee_groupby_State',\n",
    " 'img_35',\n",
    " 'ratio_var_Fee_groupby_Type_Breed1',\n",
    " 'dog_cat_scores_amin_max',\n",
    " 'dog_cat_topics_mean_median',\n",
    " 'ratio_max_Quantity_groupby_State',\n",
    " 'img_178',\n",
    " 'img_166',\n",
    " 'img_250',\n",
    " 'img_31',\n",
    " 'img_191',\n",
    " 'diff_mean_Age_groupby_Type_Breed1_Breed2',\n",
    " 'var_Sterilized_groupby_RescuerID',\n",
    " 'glove_mag276',\n",
    " 'img_151',\n",
    " 'glove_mag35',\n",
    " 'Gender',\n",
    " 'var_Quantity_groupby_RescuerID_Type',\n",
    " 'State',\n",
    " 'annots_top_desc_count_svd_4',\n",
    " 'var_Fee_groupby_Type_Breed1_Breed2',\n",
    " 'ratio_sum_Quantity_groupby_State',\n",
    " 'img_8',\n",
    " 'mean_Age_groupby_RescuerID',\n",
    " 'ratio_sum_Age_groupby_State',\n",
    " 'img_105',\n",
    " 'img_28',\n",
    " 'gnvec285',\n",
    " 'sum_Age_groupby_RescuerID',\n",
    " 'diff_median_Age_groupby_Type_Breed1_Breed2',\n",
    " 'glove_mag132',\n",
    " 'color_red_score_amax_var',\n",
    " 'img_174',\n",
    " 'ratio_mean_Fee_groupby_Type_Breed1_Breed2',\n",
    " 'crop_x_var',\n",
    " 'gnvec101',\n",
    " 'var_Age_groupby_RescuerID',\n",
    " 'img_186',\n",
    " 'diff_var_Age_groupby_RescuerID_State',\n",
    " 'fix_Breed1',\n",
    " 'img_244',\n",
    " 'gnvec34',\n",
    " 'min_Quantity_groupby_RescuerID_State',\n",
    " 'diff_sum_Age_groupby_RescuerID',\n",
    " 'img_175',\n",
    " 'img_237',\n",
    " 'gnvec15',\n",
    " 'glove_mag44',\n",
    " 'annots_score_mean_max',\n",
    " 'annots_score_mean_mean',\n",
    " 'ratio_var_Fee_groupby_Type_Breed1_Breed2',\n",
    " 'gnvec6',\n",
    " 'img_133',\n",
    " 'img_119',\n",
    " 'diff_var_Fee_groupby_RescuerID_State',\n",
    " 'img_243',\n",
    " 'img_113',\n",
    " 'img_116',\n",
    " 'img_156',\n",
    " 'gnvec282',\n",
    " 'diff_var_Fee_groupby_Type_Breed1_Breed2',\n",
    " 'img_242',\n",
    " 'img_117',\n",
    " 'img_173',\n",
    " 'gnvec27',\n",
    " 'img_4',\n",
    " 'gnvec87',\n",
    " 'glove_mag121',\n",
    " 'gnvec18',\n",
    " 'glove_mag188',\n",
    " 'img_188',\n",
    " 'img_63',\n",
    " 'img_183',\n",
    " 'img_48',\n",
    " 'glove_mag159',\n",
    " 'gnvec241',\n",
    " 'img_36',\n",
    " 'img_221',\n",
    " 'gnvec31',\n",
    " 'ratio_mean_Age_groupby_Type_Breed1',\n",
    " 'img_126',\n",
    " 'mean_Quantity_groupby_RescuerID',\n",
    " 'annots_score_amax_median',\n",
    " 'annots_top_desc_count_svd_3',\n",
    " 'gnvec230',\n",
    " 'gnvec175',\n",
    " 'glove_mag16',\n",
    " 'glove_mag6',\n",
    " 'glove_mag57',\n",
    " 'diff_var_Age_groupby_Type_Breed1',\n",
    " 'annots_score_amin_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "     'Breed1',\n",
    "     'Breed2',\n",
    "     'Color1',\n",
    "     'Color2',\n",
    "     'Color3',\n",
    "     'Dewormed',\n",
    "     'FurLength',\n",
    "     'Gender',\n",
    "     'Health',\n",
    "     'MaturitySize',\n",
    "     'State',\n",
    "     'Sterilized',\n",
    "     'Type',\n",
    "     'Vaccinated',\n",
    "     'Type_main_breed',\n",
    "     'BreedName_main_breed',\n",
    "     'Type_second_breed',\n",
    "     'BreedName_second_breed',\n",
    "]\n",
    "max_features=128\n",
    "\n",
    "X_train = feather.read_dataframe('X_train9.feather')\n",
    "n_train = len(X_train)\n",
    "img_cols = [\"img_{}\".format(i) for i in range(256)]\n",
    "numerical = [c for c in X_train.columns if c not in categorical_features and c not in img_cols]\n",
    "numerical = [c for c in numerical if c in use_cols]\n",
    "\n",
    "y =  feather.read_dataframe('../input/X_train.feather')[\"AdoptionSpeed\"].values\n",
    "rescuer_id = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv').loc[:, 'RescuerID'].iloc[:n_train]\n",
    "\n",
    "embedding = \"../input/pymagnitude-data/glove.840B.300d.magnitude\"\n",
    "train = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\n",
    "train[['Description', 'Name']] = train[['Description', 'Name']].astype(str)\n",
    "train[\"Description\"] = [analyzer(text) for text in train[\"Description\"]]\n",
    "X_desc, embedding_dim = w2v_pymagnitude_tonn(train[\"Description\"][:n_train], embedding, max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "for c in categorical_features:\n",
    "    X_train[c] = LabelEncoder().fit_transform(X_train[c])\n",
    "X_train.replace(np.inf, np.nan, inplace=True)\n",
    "X_train.replace(-np.inf, np.nan, inplace=True)\n",
    "X_train[numerical] = StandardScaler().fit_transform(X_train[numerical].rank())\n",
    "X_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import InputSpec\n",
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "    \n",
    "def get_model(max_features, embedding_dim, emb_n=5, dout=.4, weight_decay=0.1):\n",
    "    inp_cats = []\n",
    "    embs = []\n",
    "    for c in categorical_features:\n",
    "        inp_cat = Input(shape=[1], name=c)\n",
    "        inp_cats.append(inp_cat)\n",
    "        embs.append((Embedding(X_train[c].max()+1, emb_n)(inp_cat)))\n",
    "    cats = Flatten()(concatenate(embs))\n",
    "    cats = Dense(8, activation=\"relu\")(cats)\n",
    "    cats = Dropout(dout)(cats)\n",
    "    cats = BatchNormalization()(cats)\n",
    "    \n",
    "    inp_numerical =  Input(shape=(len(numerical),), name=\"numerical\")\n",
    "    nums = Dense(256, activation=\"relu\")(inp_numerical)\n",
    "    nums = Dropout(dout)(nums)\n",
    "    nums = BatchNormalization()(nums)\n",
    "    \n",
    "    inp_img =  Input(shape=(len(img_cols),), name=\"img\")\n",
    "    x_img = BatchNormalization()(inp_img)\n",
    "    \n",
    "    inp_desc = Input(shape=(max_features, embedding_dim), name=\"description\")\n",
    "    emb_desc = SpatialDropout1D(0.3)(inp_desc)\n",
    "    x1 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(emb_desc)\n",
    "    x2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)\n",
    "\n",
    "    \n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "    avg_pool2 = GlobalAveragePooling1D()(x2)\n",
    "    #attn = AttentionWeightedAverage()(x1)\n",
    "    #att1 = Attention(max_features)(x1)\n",
    "    #att2 = Attention(max_features)(x2)\n",
    "    conc = Concatenate()([max_pool1, max_pool2, avg_pool1, avg_pool2])\n",
    "    conc = BatchNormalization()(conc)\n",
    "    \n",
    "    x = concatenate([conc, x_img, nums, cats])\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dout/2)(x)\n",
    "    \n",
    "    out = Dense(1, activation=\"linear\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp_cats+[inp_numerical, inp_img, inp_desc], outputs=out)\n",
    "    #model.compile(optimizer=AdamW(weight_decay=weight_decay), loss=rmse)\n",
    "    model.compile(optimizer=\"adam\", loss=rmse)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11994 samples, validate on 2999 samples\n",
      "Epoch 1/25\n",
      "11994/11994 [==============================] - 12s 986us/step - loss: 1.3226 - val_loss: 7.0443\n",
      "Epoch 2/25\n",
      "11994/11994 [==============================] - 9s 774us/step - loss: 0.9442 - val_loss: 0.9408\n",
      "Epoch 3/25\n",
      "11994/11994 [==============================] - 9s 785us/step - loss: 0.8648 - val_loss: 0.8501\n",
      "Epoch 4/25\n",
      "11994/11994 [==============================] - 9s 779us/step - loss: 0.8654 - val_loss: 1.0138\n",
      "Epoch 5/25\n",
      "11994/11994 [==============================] - 9s 782us/step - loss: 0.8445 - val_loss: 0.8754\n",
      "Epoch 6/25\n",
      "11994/11994 [==============================] - 9s 780us/step - loss: 0.8240 - val_loss: 0.8955\n",
      "Epoch 7/25\n",
      "11994/11994 [==============================] - 9s 778us/step - loss: 0.8306 - val_loss: 1.9302\n",
      "Epoch 8/25\n",
      "11994/11994 [==============================] - 9s 774us/step - loss: 0.8033 - val_loss: 1.0935\n",
      "Epoch 9/25\n",
      "11994/11994 [==============================] - 9s 769us/step - loss: 0.7980 - val_loss: 0.8631\n",
      "Epoch 10/25\n",
      "11994/11994 [==============================] - 9s 774us/step - loss: 0.7877 - val_loss: 0.8501\n",
      "Epoch 11/25\n",
      "11994/11994 [==============================] - 9s 774us/step - loss: 0.7762 - val_loss: 0.9516\n",
      "Epoch 12/25\n",
      "11994/11994 [==============================] - 9s 775us/step - loss: 0.7715 - val_loss: 0.8572\n",
      "Epoch 13/25\n",
      "11994/11994 [==============================] - 9s 775us/step - loss: 0.7599 - val_loss: 0.9062\n",
      "Epoch 14/25\n",
      "11994/11994 [==============================] - 9s 779us/step - loss: 0.7557 - val_loss: 1.0038\n",
      "Epoch 15/25\n",
      "11994/11994 [==============================] - 9s 773us/step - loss: 0.7524 - val_loss: 0.8609\n",
      "Epoch 16/25\n",
      "11994/11994 [==============================] - 9s 771us/step - loss: 0.7443 - val_loss: 0.8714\n",
      "Epoch 17/25\n",
      "11994/11994 [==============================] - 9s 773us/step - loss: 0.7397 - val_loss: 0.9087\n",
      "Epoch 18/25\n",
      "11994/11994 [==============================] - 9s 773us/step - loss: 0.7337 - val_loss: 0.8926\n",
      "Epoch 19/25\n",
      "11994/11994 [==============================] - 9s 772us/step - loss: 0.7263 - val_loss: 1.1022\n",
      "Epoch 20/25\n",
      "11994/11994 [==============================] - 9s 772us/step - loss: 0.7216 - val_loss: 0.8759\n",
      "Epoch 21/25\n",
      "11994/11994 [==============================] - 9s 768us/step - loss: 0.7129 - val_loss: 0.8697\n",
      "Epoch 22/25\n",
      "11994/11994 [==============================] - 9s 770us/step - loss: 0.7057 - val_loss: 0.9056\n",
      "Epoch 23/25\n",
      "11994/11994 [==============================] - 9s 772us/step - loss: 0.6999 - val_loss: 0.8911\n",
      "Epoch 24/25\n",
      " 1152/11994 [=>............................] - ETA: 7s - loss: 0.6999"
     ]
    }
   ],
   "source": [
    "n_splits=5\n",
    "cv = GroupKFold(n_splits=n_splits)\n",
    "avg_valid_kappa = 0\n",
    "batch_size=128\n",
    "coeffs=None\n",
    "\n",
    "#x_test = get_keras_data(test_df, desc_embs[len(train_df):])\n",
    "#y_nn_test = np.zeros((len(test_df),))\n",
    "y_nn_oof = np.zeros((X_train.shape[0]))\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(cv.split(range(len(X_train)), y=None, groups=rescuer_id)):\n",
    "    x_train = get_keras_data(X_train.iloc[train_idx], X_desc[train_idx])\n",
    "    x_valid = get_keras_data(X_train.iloc[valid_idx], X_desc[valid_idx])\n",
    "    y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "    \n",
    "    model = get_model(max_features, embedding_dim)\n",
    "    clr_tri = CyclicLR(base_lr=2e-3, max_lr=4e-2, step_size=len(X_train)//batch_size, mode=\"triangular2\")\n",
    "    ckpt = ModelCheckpoint('model.hdf5', save_best_only=True,\n",
    "                               monitor='val_loss', mode='min')\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, validation_data=(x_valid, y_valid), \n",
    "                        epochs=25, callbacks=[ckpt, clr_tri])\n",
    "    model.load_weights('model.hdf5')\n",
    "    \n",
    "    y_pred = model.predict(x_valid, batch_size=1000).reshape(-1,)\n",
    "    y_nn_oof[valid_idx] = y_pred\n",
    "    #y_nn_test += model.predict(x_test, batch_size=batch_size).reshape(-1,) / n_splits\n",
    "    print(\"Fold{} rmse={}\".format(i, np.sqrt(mean_squared_error(y_valid, y_pred))))\n",
    "\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(y_nn_oof, y)\n",
    "coefficients = optR.coefficients()\n",
    "y_nn_oof_opt = optR.predict(y_nn_oof, coefficients)\n",
    "score = get_score(y, y_nn_oof_opt)\n",
    "print(np.sqrt(mean_squared_error(y, y_nn_oof)), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0726994102895673 0.4340790356296971\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mean_squared_error(y, y_nn_oof)), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with\n",
    "        return_sequences = True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
