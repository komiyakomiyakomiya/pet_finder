{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/keras/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import feather\n",
    "import pandas as pd\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models.fasttext import FastText\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPooling1D, concatenate, BatchNormalization, PReLU\n",
    "from keras.layers import Reshape, Flatten, Concatenate, SpatialDropout1D, GlobalAveragePooling1D, Multiply\n",
    "from keras.optimizers import Adam, Optimizer\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from pymagnitude import *\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from scipy.stats import rankdata\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "\n",
    "\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "lc = nltk.stem.lancaster.LancasterStemmer()\n",
    "sb = nltk.stem.snowball.SnowballStemmer('english')\n",
    "\n",
    "def analyzer_embed(text):\n",
    "    text = text.lower() # 小文字化\n",
    "    text = text.replace('\\n', '') # 改行削除\n",
    "    text = text.replace('\\t', '') # タブ削除\n",
    "    puncts = r',.\":)(-!?|;\\'$&/[]>%=#*+\\\\•~@£·_{}©^®`<→°€™›♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√。【】'\n",
    "    for punct in puncts:\n",
    "        text = text.replace(punct, f' {punct} ')\n",
    "    for bad_word in contraction_mapping:\n",
    "        if bad_word in text:\n",
    "            text = text.replace(bad_word, contraction_mapping[bad_word])\n",
    "    text = text.split(' ') # スペースで区切る\n",
    "    \n",
    "    words = []\n",
    "    for word in text:\n",
    "        if (re.compile(r'^.*[0-9]+.*$').fullmatch(word) is not None): # 数字が含まれるものは分割\n",
    "            for w in re.findall(r'(\\d+|\\D+)', word):\n",
    "                words.append(w)\n",
    "            continue\n",
    "        if len(word) < 1: #  0文字（空文字）は除外\n",
    "            continue\n",
    "        words.append(word)\n",
    "        \n",
    "    return \" \".join(words)\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "    \n",
    "def to_bins(x, borders):\n",
    "    for i in range(len(borders)):\n",
    "        if x <= borders[i]:\n",
    "            return i\n",
    "    return len(borders)\n",
    "    \n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _loss(self, coef, X, y, idx):\n",
    "        X_p = np.array([to_bins(pred, coef) for pred in X])\n",
    "        ll = -get_score(y, X_p)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        coef = [0.2, 0.4, 0.6, 0.8]\n",
    "        golden1 = 0.618\n",
    "        golden2 = 1 - golden1\n",
    "        ab_start = [(0.01, 0.3), (0.15, 0.56), (0.35, 0.75), (0.6, 0.9)]\n",
    "        for it1 in range(10):\n",
    "            for idx in range(4):\n",
    "                # golden section search\n",
    "                a, b = ab_start[idx]\n",
    "                # calc losses\n",
    "                coef[idx] = a\n",
    "                la = self._loss(coef, X, y, idx)\n",
    "                coef[idx] = b\n",
    "                lb = self._loss(coef, X, y, idx)\n",
    "                for it in range(20):\n",
    "                    # choose value\n",
    "                    if la > lb:\n",
    "                        a = b - (b - a) * golden1\n",
    "                        coef[idx] = a\n",
    "                        la = self._loss(coef, X, y, idx)\n",
    "                    else:\n",
    "                        b = b - (b - a) * golden2\n",
    "                        coef[idx] = b\n",
    "                        lb = self._loss(coef, X, y, idx)\n",
    "        self.coef_ = {'x': coef}\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.array([to_bins(pred, coef) for pred in X])\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "    \n",
    "class StratifiedGroupKFold():\n",
    "    def __init__(self, n_splits=5):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        fold = pd.DataFrame([X, y, groups]).T\n",
    "        fold.columns = ['X', 'y', 'groups']\n",
    "        fold['y'] = fold['y'].astype(int)\n",
    "        g = fold.groupby('groups')['y'].agg('mean').reset_index()\n",
    "        fold = fold.merge(g, how='left', on='groups', suffixes=('', '_mean'))\n",
    "        fold['y_mean'] = fold['y_mean'].apply(np.round)\n",
    "        fold['fold_id'] = 0\n",
    "        for unique_y in fold['y_mean'].unique():\n",
    "            mask = fold.y_mean==unique_y\n",
    "            selected = fold[mask].reset_index(drop=True)\n",
    "            cv = GroupKFold(n_splits=n_splits)\n",
    "            for i, (train_index, valid_index) in enumerate(cv.split(range(len(selected)), y=None, groups=selected['groups'])):\n",
    "                selected.loc[valid_index, 'fold_id'] = i\n",
    "            fold.loc[mask, 'fold_id'] = selected['fold_id'].values\n",
    "            \n",
    "        for i in range(self.n_splits):\n",
    "            indices = np.arange(len(fold))\n",
    "            train_index = indices[fold['fold_id'] != i]\n",
    "            valid_index = indices[fold['fold_id'] == i]\n",
    "            yield train_index, valid_index\n",
    "    \n",
    "def get_keras_data(df, description_embeds):\n",
    "    X = {\n",
    "        \"numerical\": df[numerical].values,\n",
    "        \"important_numerical\": df[important_numerical].values,\n",
    "        \"description\": description_embeds,\n",
    "        \"dense_cols\": df[dense_cols],\n",
    "        \"inception_cols\": df[inception_cols]\n",
    "    }\n",
    "    for c in categorical_features + important_categorical:\n",
    "        X[c] = df[c]\n",
    "    return X\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y-y_pred), axis=-1))\n",
    "\n",
    "def w2v_fornn(train_text, model, max_len):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(list(train_text))\n",
    "    train_text = tokenizer.texts_to_sequences(train_text)\n",
    "    train_text = pad_sequences(train_text, maxlen=max_len)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    embedding_dim = model.dim\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "    \n",
    "    result = []\n",
    "    for word, i in word_index.items():\n",
    "        if word in model:  # 0.9906\n",
    "            embedding_matrix[i] = model.query(word)\n",
    "            continue\n",
    "        word_ = word.upper()\n",
    "        if word_ in model:  # 0.9909\n",
    "            embedding_matrix[i] = model.query(word_)\n",
    "            continue\n",
    "        word_ = word.capitalize()\n",
    "        if word_ in model:  # 0.9925\n",
    "            embedding_matrix[i] = model.query(word_)\n",
    "            continue\n",
    "        word_ = ps.stem(word)\n",
    "        if word_ in model:  # 0.9927\n",
    "            embedding_matrix[i] = model.query(word_)\n",
    "            continue\n",
    "        word_ = lc.stem(word)\n",
    "        if word_ in model:  # 0.9932\n",
    "            embedding_matrix[i] = model.query(word_)\n",
    "            continue\n",
    "        word_ = sb.stem(word)\n",
    "        if word_ in model:  # 0.9933\n",
    "            embedding_matrix[i] = model.query(word_)\n",
    "            continue\n",
    "        embedding_matrix[i] = model.query(word)\n",
    "\n",
    "    return train_text, embedding_matrix, embedding_dim, word_index\n",
    "\n",
    "def fasttext_fornn(train_text, model, max_len):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(list(train_text))\n",
    "    train_text = tokenizer.texts_to_sequences(train_text)\n",
    "    train_text = pad_sequences(train_text, maxlen=max_len)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    embedding_dim = model.vector_size\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "\n",
    "    result = []\n",
    "    for word, i in word_index.items():\n",
    "        if word in model:  # 0.9906\n",
    "            embedding_matrix[i] = model.wv[word]\n",
    "            continue\n",
    "        word_ = word.upper()\n",
    "        if word_ in model:  # 0.9909\n",
    "            embedding_matrix[i] = model.wv[word_]\n",
    "            continue\n",
    "        word_ = word.capitalize()\n",
    "        if word_ in model:  # 0.9925\n",
    "            embedding_matrix[i] = model.wv[word_]\n",
    "            continue\n",
    "        word_ = ps.stem(word)\n",
    "        if word_ in model:  # 0.9927\n",
    "            embedding_matrix[i] = model.wv[word_]\n",
    "            continue\n",
    "        word_ = lc.stem(word)\n",
    "        if word_ in model:  # 0.9932\n",
    "            embedding_matrix[i] = model.wv[word_]\n",
    "            continue\n",
    "        word_ = sb.stem(word)\n",
    "        if word_ in model:  # 0.9933\n",
    "            embedding_matrix[i] = model.wv[word_]\n",
    "            continue\n",
    "        embedding_matrix[i] = np.zeros(embedding_dim)\n",
    "        \n",
    "    return train_text, embedding_matrix, embedding_dim, word_index\n",
    "\n",
    "def self_train_w2v_tonn(train_text, max_len, w2v_params, mode=\"w2v\"):\n",
    "    train_corpus = [text_to_word_sequence(text) for text in train_text]\n",
    "    if mode == \"w2v\":\n",
    "        model = word2vec.Word2Vec(train_corpus, **w2v_params)\n",
    "    elif mode == \"fasttext\":\n",
    "        model = FastText(train_corpus, **w2v_params)\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(list(train_text))\n",
    "    train_text = tokenizer.texts_to_sequences(train_text)\n",
    "    train_text = pad_sequences(train_text, maxlen=max_len)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    embedding_dim = model.vector_size\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if word in model:  # 0.9906\n",
    "            embedding_matrix[i] = model.wv[word]\n",
    "            continue\n",
    "        word_ = word.upper()\n",
    "        if word_ in model:  # 0.9909\n",
    "            embedding_matrix[i] = model.wv[word_]\n",
    "            continue\n",
    "        word_ = word.capitalize()\n",
    "        if word_ in model:  # 0.9925\n",
    "            embedding_matrix[i] = model.wv[word_]\n",
    "            continue\n",
    "        word_ = ps.stem(word)\n",
    "        if word_ in model:  # 0.9927\n",
    "            embedding_matrix[i] = model.wv[word_]\n",
    "            continue\n",
    "        word_ = lc.stem(word)\n",
    "        if word_ in model:  # 0.9932\n",
    "            embedding_matrix[i] = model.wv[word_]\n",
    "            continue\n",
    "        word_ = sb.stem(word)\n",
    "        if word_ in model:  # 0.9933\n",
    "            embedding_matrix[i] = model.wv[word_]\n",
    "            continue\n",
    "        embedding_matrix[i] = np.zeros(embedding_dim)\n",
    "        \n",
    "    return train_text, embedding_matrix, embedding_dim, word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w2v_params = {\\n    \"size\": 300,\\n    \"seed\": 0,\\n    \"min_count\": 1,\\n    \"workers\": 1\\n}\\nX_desc, embedding_matrix, embedding_dim, word_index = self_train_w2v_tonn(X_train[\"Description_bow\"], max_len, w2v_params, \"fasttext\")\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = [\n",
    "     'Breed1',\n",
    "     'Breed2',\n",
    "     'Color1',\n",
    "     'Color2',\n",
    "     'Color3',\n",
    "     'Dewormed',\n",
    "     'FurLength',\n",
    "     'Gender',\n",
    "     'Health',\n",
    "     'MaturitySize',\n",
    "     'State',\n",
    "     'Sterilized',\n",
    "     'Type',\n",
    "     'Vaccinated',\n",
    "     'Type_main_breed',\n",
    "     'BreedName_main_breed',\n",
    "     'Type_second_breed',\n",
    "     'BreedName_second_breed',\n",
    "]\n",
    "max_len=128\n",
    "n_important = 100\n",
    "\n",
    "X_train = feather.read_dataframe('from_kernel/all_datav7.feather')\n",
    "ranking = feather.read_dataframe(\"from_kernel/all_datav17.feather\")[\"BreedDogRank_second\"]\n",
    "X_train[\"BreedDogRank_second\"] = ranking\n",
    "ranking = feather.read_dataframe(\"from_kernel/all_datav17.feather\")[\"BreedDogRank_main\"]\n",
    "X_train[\"BreedDogRank_main\"] = ranking\n",
    "len_train = 14993\n",
    "\n",
    "use_cols = pd.read_csv(\"importance10.csv\")\n",
    "use_cols[\"gain\"] = use_cols[\"gain\"] / use_cols[\"gain\"].sum()\n",
    "use_cols = list(use_cols[use_cols.gain>0.0002].feature.values)\n",
    "use_cols.remove(\"BreedID_y\")\n",
    "use_cols.remove(\"BreedDogRank_second\")\n",
    "use_cols.remove(\"BreedDogRank_main\")\n",
    "dense_cols = [c for c in X_train.columns if \"dense\" in c and \"svd\" not in c and \"nmf\" not in c]\n",
    "inception_cols = [c for c in X_train.columns if \"inception\" in c and \"svd\" not in c and \"nmf\" not in c]\n",
    "numerical = [c for c in use_cols if c not in categorical_features and c not in inception_cols+dense_cols]\n",
    "#numerical = [c for c in numerical if c in use_cols]\n",
    "\n",
    "important_numerical = [c for c in numerical if c in use_cols[:n_important]]\n",
    "numerical = [c for c in numerical if c not in use_cols[:n_important]]\n",
    "important_categorical = [c for c in categorical_features if c in use_cols[:n_important]]\n",
    "categorical_features = [c for c in categorical_features if c not in use_cols[:n_important]]\n",
    "\n",
    "y =  feather.read_dataframe('../input/X_train.feather')[\"AdoptionSpeed\"].values\n",
    "rescuer_id = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv').loc[:, 'RescuerID'].iloc[:len_train]\n",
    "\n",
    "#embedding = '../input/quora-embedding/GoogleNews-vectors-negative300.bin'\n",
    "#model = KeyedVectors.load_word2vec_format(embedding, binary=True)\n",
    "#X_desc, embedding_matrix, embedding_dim, word_index  = fasttext_fornn(X_train[\"Description_Emb\"], model, max_len)\n",
    "\n",
    "embedding = \"../input/pymagnitude-data/glove.840B.300d.magnitude\"\n",
    "model = Magnitude(embedding)\n",
    "X_desc, embedding_matrix, embedding_dim, word_index = w2v_fornn(X_train[\"Description_Emb\"], model, max_len)\n",
    "\n",
    "\"\"\"w2v_params = {\n",
    "    \"size\": 300,\n",
    "    \"seed\": 0,\n",
    "    \"min_count\": 1,\n",
    "    \"workers\": 1\n",
    "}\n",
    "X_desc, embedding_matrix, embedding_dim, word_index = self_train_w2v_tonn(X_train[\"Description_bow\"], max_len, w2v_params, \"fasttext\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "for c in categorical_features + important_categorical:\n",
    "    X_train[c] = LabelEncoder().fit_transform(X_train[c])\n",
    "X_train.replace(np.inf, np.nan, inplace=True)\n",
    "X_train.replace(-np.inf, np.nan, inplace=True)\n",
    "X_train[important_numerical+numerical] = StandardScaler().fit_transform(X_train[important_numerical+numerical].rank())\n",
    "X_train.fillna(0, inplace=True)\n",
    "\n",
    "X_test = X_train.iloc[len_train:]\n",
    "X_train = X_train.iloc[:len_train]\n",
    "X_desc_test = X_desc[len_train:]\n",
    "X_desc_train = X_desc[:len_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import InputSpec\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with\n",
    "        return_sequences = True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "class ShakeShake(Layer):\n",
    "    \"\"\" Shake-Shake-Image Layer \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.ndim = 2\n",
    "        super(ShakeShake, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ShakeShake, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # unpack x1 and x2\n",
    "        assert isinstance(x, list)\n",
    "        x1, x2 = x\n",
    "        # create alpha and beta\n",
    "        batch_size = K.shape(x1)[0]\n",
    "        alpha = K.random_uniform((batch_size, 1, 1, 1))\n",
    "        beta = K.random_uniform((batch_size, 1, 1, 1))\n",
    "        # shake-shake during training phase\n",
    "        def x_shake():\n",
    "            return beta * x1 + (1 - beta) * x2 + K.stop_gradient((alpha - beta) * x1 + (beta - alpha) * x2)\n",
    "        # even-even during testing phase\n",
    "        def x_even():\n",
    "            return 0.5 * x1 + 0.5 * x2\n",
    "        return K.in_train_phase(x_shake, x_even)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        return input_shape[0]\n",
    "        \n",
    "def se_block(input, channels, r=8):\n",
    "    x = Dense(channels//r, activation=\"relu\")(input)\n",
    "    x = Dense(channels, activation=\"sigmoid\")(x)\n",
    "    return Multiply()([input, x])\n",
    "    \n",
    "import keras\n",
    "\n",
    "class SWA(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, filepath, swa_epoch):\n",
    "        super(SWA, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.swa_epoch = swa_epoch \n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.nb_epoch = self.params['epochs']\n",
    "        print('Stochastic weight averaging selected for last {} epochs.'\n",
    "              .format(self.nb_epoch - self.swa_epoch))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if epoch == self.swa_epoch:\n",
    "            self.swa_weights = self.model.get_weights()\n",
    "            \n",
    "        elif epoch > self.swa_epoch:    \n",
    "            for i, layer in enumerate(self.model.layers):\n",
    "                self.swa_weights[i] = (self.swa_weights[i] * \\\n",
    "                                       (epoch - self.swa_epoch) + self.model.get_weights()[i]) \\\n",
    "                /((epoch - self.swa_epoch)  + 1)  \n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.model.set_weights(self.swa_weights)\n",
    "        print('Final model parameters set to stochastic weight average.')\n",
    "        self.model.save_weights(self.filepath)\n",
    "        print('Final stochastic averaged weights saved to file.')\n",
    "\n",
    "def get_model(max_len, embedding_dim, emb_n=4, emb_n_imp=16, dout=.5, weight_decay=0.1):\n",
    "    inp_cats = []\n",
    "    embs = []\n",
    "    for c in categorical_features:\n",
    "        inp_cat = Input(shape=[1], name=c)\n",
    "        inp_cats.append(inp_cat)\n",
    "        embs.append((Embedding(X_train[c].max()+1, emb_n)(inp_cat)))\n",
    "    for c in important_categorical:\n",
    "        inp_cat = Input(shape=[1], name=c)\n",
    "        inp_cats.append(inp_cat)\n",
    "        embs.append((Embedding(X_train[c].max()+1, emb_n_imp)(inp_cat)))\n",
    "    cats = Flatten()(concatenate(embs))\n",
    "    imp_cats = Flatten()(concatenate(embs))\n",
    "    cats = Dense(8, activation=\"linear\")(cats)\n",
    "    cats = BatchNormalization()(cats)\n",
    "    cats = PReLU()(cats)\n",
    "    cats = Dropout(dout/2)(cats)\n",
    "    \n",
    "    inp_numerical =  Input(shape=(len(numerical),), name=\"numerical\")\n",
    "    inp_important_numerical = Input(shape=(len(important_numerical),), name=\"important_numerical\")\n",
    "    nums = concatenate([inp_numerical, inp_important_numerical])\n",
    "    nums = Dense(32, activation=\"linear\")(nums)\n",
    "    nums = BatchNormalization()(nums)\n",
    "    nums = PReLU()(nums)\n",
    "    nums = Dropout(dout)(nums)\n",
    "    \n",
    "    inp_dense =  Input(shape=(len(dense_cols),), name=\"dense_cols\")\n",
    "    x_dense = Dense(16, activation=\"linear\")(inp_dense)\n",
    "    x_dense = BatchNormalization()(x_dense)\n",
    "    x_dense = PReLU()(x_dense)\n",
    "    \n",
    "    inp_inception =  Input(shape=(len(inception_cols),), name=\"inception_cols\")\n",
    "    x_inception = Dense(16, activation=\"linear\")(inp_inception)\n",
    "    x_inception = BatchNormalization()(x_inception)\n",
    "    x_inception = PReLU()(x_inception)\n",
    "    \n",
    "    x_img = concatenate([x_dense, x_inception])\n",
    "    x_img = Dense(32, activation=\"linear\")(x_img)\n",
    "    x_img = BatchNormalization()(x_img)\n",
    "    x_img = PReLU()(x_img)\n",
    "    x_img = Dropout(dout)(x_img)\n",
    "    \n",
    "    inp_desc = Input(shape=(max_len, ), name=\"description\")\n",
    "    emb_desc = Embedding(len(embedding_matrix), embedding_dim, weights=[embedding_matrix], trainable=False)(inp_desc)\n",
    "    emb_desc = SpatialDropout1D(0.2)(emb_desc)\n",
    "    x1 = Bidirectional(CuDNNLSTM(32, return_sequences=True))(emb_desc)\n",
    "    x2 = Bidirectional(CuDNNGRU(32, return_sequences=True))(x1)\n",
    "    #x2 = Conv1D(64, 1)(x1)\n",
    "    \n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    avg_pool2 = GlobalAveragePooling1D()(x2)\n",
    "    att2 = Attention(max_len)(x2)\n",
    "    conc = Concatenate()([max_pool2, avg_pool2, att2])\n",
    "    conc = se_block(conc,64+64+64)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    \n",
    "    conc = Dense(32, activation=\"linear\")(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = PReLU()(conc)\n",
    "    conc = Dropout(dout)(conc)\n",
    "    \n",
    "    x = concatenate([conc, x_img, nums, cats, inp_important_numerical])\n",
    "    x = se_block(x,32+32+32+8+len(important_numerical))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dout)(x)\n",
    "    x = concatenate([x, inp_important_numerical])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dout/2)(x)\n",
    "    \n",
    "    out = Dense(1, activation=\"linear\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp_cats+[inp_numerical, inp_important_numerical, inp_dense, inp_inception, inp_desc], outputs=out)\n",
    "    model.compile(optimizer=\"adam\", loss=rmse)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11992 samples, validate on 3001 samples\n",
      "Stochastic weight averaging selected for last 2 epochs.\n",
      "Epoch 1/20\n",
      "  512/11992 [>.............................] - ETA: 3:37 - loss: 2.6857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/keras/callbacks.py:122: UserWarning: In your callbacks, method `on_batch_end()` is slow compared to a model step (1.617340 vs 0.070967). Check your callbacks.\n",
      "  % (delta_t_median, self._delta_t_batch))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11992/11992 [==============================] - 24s 2ms/step - loss: 1.7404 - val_loss: 0.9734\n",
      "Epoch 2/20\n",
      "11992/11992 [==============================] - 11s 924us/step - loss: 0.9915 - val_loss: 0.8782\n",
      "Epoch 3/20\n",
      "11992/11992 [==============================] - 11s 936us/step - loss: 0.9279 - val_loss: 0.8562\n",
      "Epoch 4/20\n",
      "11992/11992 [==============================] - 11s 916us/step - loss: 0.9142 - val_loss: 0.8958\n",
      "Epoch 5/20\n",
      "11992/11992 [==============================] - 11s 933us/step - loss: 0.8943 - val_loss: 0.8585\n",
      "Epoch 6/20\n",
      "11992/11992 [==============================] - 11s 931us/step - loss: 0.8879 - val_loss: 0.8543\n",
      "Epoch 7/20\n",
      "11992/11992 [==============================] - 11s 921us/step - loss: 0.8776 - val_loss: 0.8469\n",
      "Epoch 8/20\n",
      "11992/11992 [==============================] - 11s 939us/step - loss: 0.8638 - val_loss: 0.8517\n",
      "Epoch 9/20\n",
      "11992/11992 [==============================] - 11s 931us/step - loss: 0.8578 - val_loss: 0.8495\n",
      "Epoch 10/20\n",
      "11992/11992 [==============================] - 11s 928us/step - loss: 0.8587 - val_loss: 0.8523\n",
      "Epoch 11/20\n",
      "11992/11992 [==============================] - 11s 931us/step - loss: 0.8496 - val_loss: 0.8524\n",
      "Epoch 12/20\n",
      "11992/11992 [==============================] - 11s 925us/step - loss: 0.8449 - val_loss: 0.8637\n",
      "Epoch 13/20\n",
      "11992/11992 [==============================] - 11s 925us/step - loss: 0.8415 - val_loss: 0.8557\n",
      "Epoch 14/20\n",
      "11992/11992 [==============================] - 11s 936us/step - loss: 0.8451 - val_loss: 0.8528\n",
      "Epoch 15/20\n",
      "11992/11992 [==============================] - 11s 924us/step - loss: 0.8436 - val_loss: 0.8523\n",
      "Epoch 16/20\n",
      "11992/11992 [==============================] - 11s 942us/step - loss: 0.8419 - val_loss: 0.8511\n",
      "Epoch 17/20\n",
      "11992/11992 [==============================] - 11s 928us/step - loss: 0.8373 - val_loss: 0.8536\n",
      "Epoch 18/20\n",
      "11992/11992 [==============================] - 11s 937us/step - loss: 0.8391 - val_loss: 0.8521\n",
      "Epoch 19/20\n",
      "11992/11992 [==============================] - 11s 938us/step - loss: 0.8419 - val_loss: 0.8531\n",
      "Epoch 20/20\n",
      "11992/11992 [==============================] - 11s 931us/step - loss: 0.8332 - val_loss: 0.8525\n",
      "Final model parameters set to stochastic weight average.\n",
      "Final stochastic averaged weights saved to file.\n",
      "Fold0 rmse=1.0403989991734457\n",
      "Train on 11993 samples, validate on 3000 samples\n",
      "Stochastic weight averaging selected for last 2 epochs.\n",
      "Epoch 1/20\n",
      "  512/11993 [>.............................] - ETA: 3:42 - loss: 2.6510"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/keras/callbacks.py:122: UserWarning: In your callbacks, method `on_batch_end()` is slow compared to a model step (1.655146 vs 0.083347). Check your callbacks.\n",
      "  % (delta_t_median, self._delta_t_batch))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11993/11993 [==============================] - 24s 2ms/step - loss: 1.7313 - val_loss: 1.7090\n",
      "Epoch 2/20\n",
      "11993/11993 [==============================] - 11s 930us/step - loss: 0.9809 - val_loss: 0.9429\n",
      "Epoch 3/20\n",
      "11993/11993 [==============================] - 11s 934us/step - loss: 0.9110 - val_loss: 0.9039\n",
      "Epoch 4/20\n",
      "11993/11993 [==============================] - 11s 943us/step - loss: 0.8999 - val_loss: 0.9137\n",
      "Epoch 5/20\n",
      "11993/11993 [==============================] - 11s 939us/step - loss: 0.8846 - val_loss: 0.9116\n",
      "Epoch 6/20\n",
      "11993/11993 [==============================] - 11s 939us/step - loss: 0.8668 - val_loss: 0.8960\n",
      "Epoch 7/20\n",
      "11993/11993 [==============================] - 11s 932us/step - loss: 0.8662 - val_loss: 0.8873\n",
      "Epoch 8/20\n",
      "11993/11993 [==============================] - 11s 955us/step - loss: 0.8490 - val_loss: 0.8871\n",
      "Epoch 9/20\n",
      "11993/11993 [==============================] - 11s 945us/step - loss: 0.8509 - val_loss: 0.8956\n",
      "Epoch 10/20\n",
      "11993/11993 [==============================] - 11s 935us/step - loss: 0.8439 - val_loss: 0.8927\n",
      "Epoch 11/20\n",
      "11993/11993 [==============================] - 11s 946us/step - loss: 0.8352 - val_loss: 0.8819\n",
      "Epoch 12/20\n",
      "11993/11993 [==============================] - 11s 940us/step - loss: 0.8366 - val_loss: 0.8878\n",
      "Epoch 13/20\n",
      "11993/11993 [==============================] - 11s 945us/step - loss: 0.8342 - val_loss: 0.8857\n",
      "Epoch 14/20\n",
      "11993/11993 [==============================] - 11s 944us/step - loss: 0.8368 - val_loss: 0.8792\n",
      "Epoch 15/20\n",
      "11993/11993 [==============================] - 11s 932us/step - loss: 0.8281 - val_loss: 0.8782\n",
      "Epoch 16/20\n",
      "11993/11993 [==============================] - 11s 940us/step - loss: 0.8299 - val_loss: 0.8824\n",
      "Epoch 17/20\n",
      "11993/11993 [==============================] - 11s 952us/step - loss: 0.8302 - val_loss: 0.8848\n",
      "Epoch 18/20\n",
      "11136/11993 [==========================>...] - ETA: 0s - loss: 0.8316Train on 11995 samples, validate on 2998 samples\n",
      "Stochastic weight averaging selected for last 2 epochs.\n",
      "Epoch 1/20\n",
      "  512/11995 [>.............................] - ETA: 3:45 - loss: 2.6936"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/keras/callbacks.py:122: UserWarning: In your callbacks, method `on_batch_end()` is slow compared to a model step (1.680303 vs 0.076021). Check your callbacks.\n",
      "  % (delta_t_median, self._delta_t_batch))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11995/11995 [==============================] - 25s 2ms/step - loss: 1.7473 - val_loss: 1.0058\n",
      "Epoch 2/20\n",
      "11995/11995 [==============================] - 11s 944us/step - loss: 0.9803 - val_loss: 0.9173\n",
      "Epoch 3/20\n",
      "11995/11995 [==============================] - 11s 951us/step - loss: 0.9242 - val_loss: 0.8756\n",
      "Epoch 4/20\n",
      "11995/11995 [==============================] - 11s 937us/step - loss: 0.9082 - val_loss: 0.8902\n",
      "Epoch 5/20\n",
      "11995/11995 [==============================] - 11s 941us/step - loss: 0.8907 - val_loss: 0.8720\n",
      "Epoch 6/20\n",
      "11995/11995 [==============================] - 11s 949us/step - loss: 0.8757 - val_loss: 0.8674\n",
      "Epoch 7/20\n",
      "11995/11995 [==============================] - 11s 943us/step - loss: 0.8717 - val_loss: 0.8697\n",
      "Epoch 8/20\n",
      "11995/11995 [==============================] - 11s 946us/step - loss: 0.8621 - val_loss: 0.8661\n",
      "Epoch 9/20\n",
      "11995/11995 [==============================] - 11s 937us/step - loss: 0.8558 - val_loss: 0.8677\n",
      "Epoch 10/20\n",
      "11995/11995 [==============================] - 11s 945us/step - loss: 0.8472 - val_loss: 0.8651\n",
      "Epoch 11/20\n",
      "11995/11995 [==============================] - 11s 945us/step - loss: 0.8433 - val_loss: 0.8679\n",
      "Epoch 12/20\n",
      "11995/11995 [==============================] - 11s 932us/step - loss: 0.8398 - val_loss: 0.8651\n",
      "Epoch 13/20\n",
      "11995/11995 [==============================] - 11s 956us/step - loss: 0.8387 - val_loss: 0.8607\n",
      "Epoch 14/20\n",
      "11995/11995 [==============================] - 11s 947us/step - loss: 0.8438 - val_loss: 0.8618\n",
      "Epoch 15/20\n",
      "11995/11995 [==============================] - 11s 940us/step - loss: 0.8388 - val_loss: 0.8591\n",
      "Epoch 16/20\n",
      "11995/11995 [==============================] - 11s 943us/step - loss: 0.8382 - val_loss: 0.8616\n",
      "Epoch 17/20\n",
      "11995/11995 [==============================] - 11s 950us/step - loss: 0.8293 - val_loss: 0.8606\n",
      "Epoch 18/20\n",
      "11995/11995 [==============================] - 11s 951us/step - loss: 0.8428 - val_loss: 0.8603\n",
      "Epoch 19/20\n",
      "11995/11995 [==============================] - 11s 946us/step - loss: 0.8305 - val_loss: 0.8595\n",
      "Epoch 20/20\n",
      "11995/11995 [==============================] - 11s 942us/step - loss: 0.8386 - val_loss: 0.8607\n",
      "Final model parameters set to stochastic weight average.\n",
      "Final stochastic averaged weights saved to file.\n",
      "Fold2 rmse=1.0567077347760256\n",
      "Train on 11995 samples, validate on 2998 samples\n",
      "Stochastic weight averaging selected for last 2 epochs.\n",
      "Epoch 1/20\n",
      "  512/11995 [>.............................] - ETA: 3:51 - loss: 2.6963"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/keras/callbacks.py:122: UserWarning: In your callbacks, method `on_batch_end()` is slow compared to a model step (1.747201 vs 0.077758). Check your callbacks.\n",
      "  % (delta_t_median, self._delta_t_batch))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11995/11995 [==============================] - 25s 2ms/step - loss: 1.7520 - val_loss: 0.9001\n",
      "Epoch 2/20\n",
      "11995/11995 [==============================] - 12s 960us/step - loss: 0.9912 - val_loss: 0.8752\n",
      "Epoch 3/20\n",
      "11995/11995 [==============================] - 11s 956us/step - loss: 0.9175 - val_loss: 0.8647\n",
      "Epoch 4/20\n",
      "11995/11995 [==============================] - 12s 966us/step - loss: 0.9141 - val_loss: 0.8572\n",
      "Epoch 5/20\n",
      "11995/11995 [==============================] - 12s 961us/step - loss: 0.8929 - val_loss: 0.8515\n",
      "Epoch 6/20\n",
      "11995/11995 [==============================] - 11s 947us/step - loss: 0.8719 - val_loss: 0.8517\n",
      "Epoch 7/20\n",
      "11995/11995 [==============================] - 11s 946us/step - loss: 0.8672 - val_loss: 0.8623\n",
      "Epoch 8/20\n",
      "11995/11995 [==============================] - 12s 960us/step - loss: 0.8572 - val_loss: 0.8476\n",
      "Epoch 9/20\n",
      "11995/11995 [==============================] - 11s 943us/step - loss: 0.8615 - val_loss: 0.8504\n",
      "Epoch 10/20\n",
      "11995/11995 [==============================] - 11s 951us/step - loss: 0.8489 - val_loss: 0.8468\n",
      "Epoch 11/20\n",
      "11995/11995 [==============================] - 11s 949us/step - loss: 0.8468 - val_loss: 0.8448\n",
      "Epoch 12/20\n",
      " 9856/11995 [=======================>......] - ETA: 1s - loss: 0.8425"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_splits=5\n",
    "avg_valid_kappa = 0\n",
    "batch_size=128\n",
    "epochs = 20\n",
    "coeffs=None\n",
    "\n",
    "x_test = get_keras_data(X_test, X_desc_test)\n",
    "y_nn_test = np.zeros((len(X_test),))\n",
    "y_nn_oof = np.zeros((X_train.shape[0]))\n",
    "\n",
    "cv = StratifiedGroupKFold(n_splits=n_splits)\n",
    "for fold_id, (train_idx, valid_idx) in enumerate(cv.split(range(len(X_train)), y=y, groups=rescuer_id)): \n",
    "    x_train = get_keras_data(X_train.iloc[train_idx], X_desc_train[train_idx])\n",
    "    x_valid = get_keras_data(X_train.iloc[valid_idx], X_desc_train[valid_idx])\n",
    "    y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "    \n",
    "    model = get_model(max_len, embedding_dim)\n",
    "    clr_tri = CyclicLR(base_lr=1e-5, max_lr=1e-2, step_size=len(X_train)//batch_size, mode=\"triangular2\")\n",
    "    ckpt = ModelCheckpoint('model.hdf5', save_best_only=True,\n",
    "                               monitor='val_loss', mode='min')\n",
    "    swa = SWA(\"swa.hdf5\", epochs-2)\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, validation_data=(x_valid, y_valid), \n",
    "                        epochs=epochs, callbacks=[ckpt, clr_tri, swa])\n",
    "    model.load_weights('model.hdf5')\n",
    "    \n",
    "    y_pred = model.predict(x_valid, batch_size=1000).reshape(-1,)\n",
    "    rmse_ = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "    y_pred = rankdata(y_pred)/len(y_pred)\n",
    "    y_nn_oof[valid_idx] = y_pred\n",
    "    \n",
    "    y_pred_test = model.predict(x_test, batch_size=1000).reshape(-1,)\n",
    "    y_pred_test = rankdata(y_pred_test)/len(y_pred_test)\n",
    "    y_nn_test += y_pred_test / n_splits\n",
    "    print(\"Fold{} rmse={}\".format(fold_id, rmse_))\n",
    "\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(y_nn_oof, y)\n",
    "coefficients = optR.coefficients()\n",
    "y_nn_oof_opt = optR.predict(y_nn_oof, coefficients)\n",
    "score = get_score(y, y_nn_oof_opt)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44622823887866003\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"y_nn_oof_nn10_451selffast.npy\", y_nn_oof)\n",
    "np.save(\"y_nn_test_nn10_451selffast.npy\", y_nn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "description (InputLayer)        (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 128, 300)     7245000     description[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 128, 300)     0           embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128, 64)      85504       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128, 64)      18816       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 64)           192         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 192)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_cols (InputLayer)         (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inception_cols (InputLayer)     (None, 384)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 24)           4632        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           8224        dense_cols[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           12320       inception_cols[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Breed2 (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Color1 (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Color2 (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Color3 (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Dewormed (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FurLength (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Gender (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Health (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "MaturitySize (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "State (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Sterilized (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Type (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Vaccinated (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Type_main_breed (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "BreedName_main_breed (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Type_second_breed (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Breed1 (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "BreedName_second_breed (InputLa (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 192)          4800        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32)           128         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32)           128         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 4)         540         Breed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 4)         28          Color1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 4)         28          Color2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 4)         24          Color3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 4)         12          Dewormed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 4)         12          FurLength[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 4)         12          Gender[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 4)         12          Health[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 4)         16          MaturitySize[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 4)         56          State[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1, 4)         12          Sterilized[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 1, 4)         8           Type[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 1, 4)         12          Vaccinated[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 1, 4)         12          Type_main_breed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 1, 4)         704         BreedName_main_breed[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 1, 4)         12          Type_second_breed[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 1, 16)        2816        Breed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 1, 16)        2160        BreedName_second_breed[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 192)          0           concatenate_5[0][0]              \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 32)           32          batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_4 (PReLU)               (None, 32)           32          batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "numerical (InputLayer)          (None, 931)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "important_numerical (InputLayer (None, 56)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 96)        0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 embedding_3[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "                                                                 embedding_5[0][0]                \n",
      "                                                                 embedding_6[0][0]                \n",
      "                                                                 embedding_7[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "                                                                 embedding_9[0][0]                \n",
      "                                                                 embedding_10[0][0]               \n",
      "                                                                 embedding_11[0][0]               \n",
      "                                                                 embedding_12[0][0]               \n",
      "                                                                 embedding_13[0][0]               \n",
      "                                                                 embedding_14[0][0]               \n",
      "                                                                 embedding_15[0][0]               \n",
      "                                                                 embedding_16[0][0]               \n",
      "                                                                 embedding_17[0][0]               \n",
      "                                                                 embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 192)          768         multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64)           0           p_re_lu_3[0][0]                  \n",
      "                                                                 p_re_lu_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 987)          0           numerical[0][0]                  \n",
      "                                                                 important_numerical[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 96)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 32)           6176        batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           31616       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            776         flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32)           128         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32)           128         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32)           128         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8)            32          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_6 (PReLU)               (None, 32)           32          batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_5 (PReLU)               (None, 32)           32          batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 32)           32          batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 8)            8           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           p_re_lu_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           p_re_lu_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8)            0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 160)          0           dropout_4[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 important_numerical[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 20)           3220        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 160)          3360        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 160)          0           concatenate_6[0][0]              \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 160)          640         multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 160)          0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 32)           5152        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 32)           5152        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32)           128         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32)           128         dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_7 (PReLU)               (None, 32)           32          batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_8 (PReLU)               (None, 32)           32          batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "shake_shake_1 (ShakeShake)      (None, 32)           0           p_re_lu_7[0][0]                  \n",
      "                                                                 p_re_lu_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 32)           0           shake_shake_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 88)           0           flatten_3[0][0]                  \n",
      "                                                                 important_numerical[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 88)           352         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 88)           0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1)            89          dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,446,505\n",
      "Trainable params: 200,161\n",
      "Non-trainable params: 7,246,344\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"y_nn_oof_nn4463.npy\", y_nn_oof)\n",
    "np.save(\"y_nn_test_nn4463.npy\", y_nn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# https://github.com/bfelbo/DeepMoji/blob/master/deepmoji/attlayer.py\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
